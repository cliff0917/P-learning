{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy.io as sio\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.manifold import TSNE\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras.optimizers import SGD, Adam\n",
    "from tensorflow.keras.layers import Input, Dense, Lambda, Reshape, Flatten, Dropout\n",
    "from tensorflow.keras.layers import Reshape, Conv2D, Conv2DTranspose, LeakyReLU\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "from process_attr import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossHistory(keras.callbacks.Callback):\n",
    "    def on_train_begin(self, logs={}):\n",
    "        self.losses = {'batch':[], 'epoch':[]}\n",
    "        self.accuracy = {'batch':[], 'epoch':[]}\n",
    "        self.val_loss = {'batch':[], 'epoch':[]}\n",
    "        self.val_acc = {'batch':[], 'epoch':[]}\n",
    "    \n",
    "    def on_batch_end(self, batch, logs={}):\n",
    "        self.losses['batch'].append(logs.get('loss'))\n",
    "        self.accuracy['batch'].append(logs.get('acc'))\n",
    "        self.val_loss['batch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['batch'].append(logs.get('val_acc'))\n",
    "    \n",
    "    def on_epoch_end(self, batch, logs={}):\n",
    "        self.losses['epoch'].append(logs.get('loss'))\n",
    "        self.accuracy['epoch'].append(logs.get('acc'))\n",
    "        self.val_loss['epoch'].append(logs.get('val_loss'))\n",
    "        self.val_acc['epoch'].append(logs.get('val_acc'))\n",
    "        \n",
    "    def loss_plot(self, loss_type):\n",
    "        iters = range(len(self.losses[loss_type]))\n",
    "        plt.figure()\n",
    "        #acc\n",
    "        plt.plot(iters, self.accuracy[loss_type], 'r', label='train acc')\n",
    "        \n",
    "        #loss\n",
    "        plt.plot(iters, self.losses[loss_type], 'g', label='train loss')\n",
    "        \n",
    "        if loss_type == 'epoch':\n",
    "            #val_acc\n",
    "            plt.plot(iters, self.val_acc[loss_type], 'b', label='val acc')\n",
    "            \n",
    "            #val_loss\n",
    "            plt.plot(iters, self.val_loss[loss_type], 'k', label='val loss')\n",
    "        \n",
    "        plt.grid(True)\n",
    "        plt.xlabel(loss_type)\n",
    "        plt.ylabel('acc-loss')\n",
    "        plt.legend(loc=\"upper right\")\n",
    "        plt.show()\n",
    "\n",
    "class Scaler(keras.layers.Layer):\n",
    "    def __init__(self, tau=0.5, **kwargs):\n",
    "        super(Scaler, self).__init__(**kwargs)\n",
    "        self.tau = tau\n",
    "    \n",
    "    def build(self, input_shape):\n",
    "        super(Scaler, self).build(input_shape)\n",
    "        self.scale = self.add_weight(\n",
    "            name='scale', shape=(input_shape[-1],), initializer='zeros'\n",
    "        )\n",
    "    \n",
    "    def call(self, inputs, mode='positive'):\n",
    "        if mode == 'positive':\n",
    "            scale = self.tau + (1 - self.tau) * K.sigmoid(self.scale)\n",
    "        else:\n",
    "            scale = (1 - self.tau) * K.sigmoid(-self.scale)\n",
    "        return inputs * K.sqrt(scale)\n",
    "    \n",
    "    def get_config(self):\n",
    "        config = {'tau': self.tau}\n",
    "        base_config = super(Scaler, self).get_config()\n",
    "        return dict(list(base_config.items()) + list(config.items()))\n",
    "\n",
    "class Sampling(keras.layers.Layer):\n",
    "    def __init__(self, latent_dim=128, **kwargs):\n",
    "        super(Sampling, self).__init__(**kwargs)\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(Sampling, self).build(input_shape)\n",
    "\n",
    "    def call(self, inputs):\n",
    "        z_mean, z_log_var = inputs\n",
    "        epsilon = K.random_normal(shape=(K.shape(z_mean)[0], self.latent_dim), seed=42)\n",
    "        return z_mean + K.exp(z_log_var / 2) * epsilon\n",
    "\n",
    "    def get_config(self):\n",
    "        base_config = super(Sampling, self).get_config()\n",
    "        config = {'latent_dim': self.latent_dim}\n",
    "        return dict(list(base_config.items()) + list(config.items()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### change dataset here ##################\n",
    "dataset = 'CUB' # 'AWA2', 'CUB', 'SUN'\n",
    "##################################################\n",
    "\n",
    "batch_size = 128\n",
    "epochs = 100\n",
    "attr_type = 'cms' # 'b', 'c', 'cmm', 'cms'\n",
    "\n",
    "file_path = f'./data/{dataset}/'\n",
    "mapping = {'b': b, 'c': c, 'cmm': cmm, 'cms': cms}\n",
    "process_attr = mapping[attr_type](file_path)\n",
    "process_attr, mat_file = np.array(process_attr)\n",
    "\n",
    "seen_class_num = sum(1 for line in open(f'{file_path}/trainvalclasses.txt') if line.rstrip())\n",
    "unseen_class_num = sum(1 for line in open(f'{file_path}/testclasses.txt') if line.rstrip())\n",
    "class_num = seen_class_num + unseen_class_num\n",
    "class_attr_dim = np.array(process_attr).shape[1]\n",
    "class_attr_shape = (class_attr_dim, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat_path = f'./data/{dataset}/mat'\n",
    "if not os.path.isdir(mat_path):\n",
    "    os.mkdir(mat_path)\n",
    "    os.mkdir(f'{mat_path}/{attr_type}')\n",
    "    os.mkdir(f'{mat_path}/{attr_type}/seen')\n",
    "    os.mkdir(f'{mat_path}/{attr_type}/unseen')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5875, 2048)\n",
      "(5875, 312)\n",
      "(5875,)\n",
      "200\n",
      "-------------------------\n",
      "(2946, 2048)\n",
      "(2946,)\n",
      "(2946,)\n",
      "186\n"
     ]
    }
   ],
   "source": [
    "features = sio.loadmat(f'{file_path}/res101.mat')\n",
    "attr = sio.loadmat(f'{file_path}/att_splits.mat')\n",
    "train_loc = attr['train_loc']\n",
    "val_loc = attr['val_loc']\n",
    "transpose_ft = features['features'].transpose()\n",
    "\n",
    "data_train = np.array([transpose_ft[i-1].flatten() for i in train_loc])\n",
    "label_train = np.array([int(features['labels'][i-1].flatten()) for i in train_loc])\n",
    "attr_train = np.array([process_attr[i-1] for i in label_train])\n",
    "print(data_train.shape)\n",
    "print(attr_train.shape)\n",
    "print(label_train.shape)\n",
    "print(max(label_train))\n",
    "\n",
    "print('-' * 25)\n",
    "\n",
    "data_val = np.array([transpose_ft[i-1].flatten() for i in val_loc])\n",
    "label_val = np.array([int(features['labels'][i-1].flatten()) for i in val_loc])\n",
    "attr_val = np.array([process_attr[i-1] for i in label_val])\n",
    "print(data_val.shape)\n",
    "print(label_val.shape)\n",
    "print(label_val.shape)\n",
    "print(max(label_val))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-13 11:14:52.213954: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-13 11:14:52.218956: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-13 11:14:52.219254: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-13 11:14:52.220284: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2022-07-13 11:14:52.220798: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-13 11:14:52.221111: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-13 11:14:52.221356: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-13 11:14:52.634214: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-13 11:14:52.634580: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-13 11:14:52.634871: I tensorflow/stream_executor/cuda/cuda_gpu_executor.cc:975] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero\n",
      "2022-07-13 11:14:52.635115: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 4780 MB memory:  -> device: 0, name: NVIDIA GeForce GTX 1060 6GB, pci bus id: 0000:01:00.0, compute capability: 6.1\n"
     ]
    }
   ],
   "source": [
    "x_inputs = Input(shape=(2048, ))\n",
    "x = x_inputs\n",
    "x = Dense(2048, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "# z_mean = Dense(class_attr_dim, activation='relu')(x)\n",
    "# z_var = Dense(class_attr_dim, activation='relu')(x)\n",
    "z_mean = Dense(class_attr_dim)(x)\n",
    "z_var = Dense(class_attr_dim)(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_inputs = Input(shape=class_attr_shape)  # expert-defined"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = Scaler()\n",
    "z_mean = scaler(z_mean, mode='positive')\n",
    "z_var = scaler(z_var, mode='negative')\n",
    "sampling = Sampling(class_attr_dim)\n",
    "z = sampling([z_mean, z_var])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Decoder "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ce_inputs = Input(shape=class_attr_shape)\n",
    "x = ce_inputs\n",
    "x = Dense(256, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(512, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(1024, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "x = Dense(2048, activation='relu')(x)\n",
    "x = BatchNormalization()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "outputs= Dense(2048, activation='relu')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder = Model(x_inputs, z)\n",
    "decoder = Model(ce_inputs, outputs)\n",
    "x_out = decoder(z)\n",
    "vae = Model(inputs=[x_inputs, y_inputs], outputs=[x_out])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# xent_loss是重構loss\n",
    "xent_loss = 0.5 * K.sum(K.mean((x_inputs - x_out)**2))\n",
    "\n",
    "# K.square(z_mean - y) 為latent v ector 向每個 class 的均值看齊 \n",
    "kl_loss = - 0.5 * K.sum(1 + z_var - K.square(z_mean - y_inputs) - K.exp(z_var), axis=-1)\n",
    "\n",
    "vae_loss = K.mean(xent_loss + kl_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "46/46 [==============================] - 3s 18ms/step - loss: 209.7118 - val_loss: 222.9103 - lr: 0.0010\n",
      "Epoch 2/100\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 116.9080 - val_loss: 79.2668 - lr: 0.0010\n",
      "Epoch 3/100\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 70.8448 - val_loss: 53.6642 - lr: 0.0010\n",
      "Epoch 4/100\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 54.0287 - val_loss: 50.0736 - lr: 0.0010\n",
      "Epoch 5/100\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 45.6168 - val_loss: 49.6719 - lr: 0.0010\n",
      "Epoch 6/100\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 39.9660 - val_loss: 49.5659 - lr: 0.0010\n",
      "Epoch 7/100\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 34.8706 - val_loss: 48.9449 - lr: 0.0010\n",
      "Epoch 8/100\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 31.6479 - val_loss: 48.5493 - lr: 0.0010\n",
      "Epoch 9/100\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 28.4615 - val_loss: 48.5969 - lr: 0.0010\n",
      "Epoch 10/100\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 26.1230 - val_loss: 48.1031 - lr: 0.0010\n",
      "Epoch 11/100\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 24.4746 - val_loss: 48.9591 - lr: 0.0010\n",
      "Epoch 12/100\n",
      "46/46 [==============================] - 1s 14ms/step - loss: 22.6034 - val_loss: 48.8124 - lr: 0.0010\n",
      "Epoch 13/100\n",
      "44/46 [===========================>..] - ETA: 0s - loss: 21.3472\n",
      "Epoch 13: ReduceLROnPlateau reducing learning rate to 0.0005000000237487257.\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 21.3330 - val_loss: 49.4748 - lr: 0.0010\n",
      "Epoch 14/100\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 19.9647 - val_loss: 48.8514 - lr: 5.0000e-04\n",
      "Epoch 15/100\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 18.9008 - val_loss: 48.6987 - lr: 5.0000e-04\n",
      "Epoch 16/100\n",
      "44/46 [===========================>..] - ETA: 0s - loss: 18.1719\n",
      "Epoch 16: ReduceLROnPlateau reducing learning rate to 0.0002500000118743628.\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 18.1375 - val_loss: 49.0804 - lr: 5.0000e-04\n",
      "Epoch 17/100\n",
      "46/46 [==============================] - 1s 15ms/step - loss: 17.4162 - val_loss: 48.7821 - lr: 2.5000e-04\n",
      "Epoch 18/100\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 16.9372 - val_loss: 48.3911 - lr: 2.5000e-04\n",
      "Epoch 19/100\n",
      "46/46 [==============================] - ETA: 0s - loss: 16.5212\n",
      "Epoch 19: ReduceLROnPlateau reducing learning rate to 0.0001250000059371814.\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 16.5212 - val_loss: 48.7750 - lr: 2.5000e-04\n",
      "Epoch 20/100\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 16.2480 - val_loss: 48.6324 - lr: 1.2500e-04\n",
      "Epoch 21/100\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 16.0450 - val_loss: 48.6313 - lr: 1.2500e-04\n",
      "Epoch 22/100\n",
      "46/46 [==============================] - ETA: 0s - loss: 15.9710\n",
      "Epoch 22: ReduceLROnPlateau reducing learning rate to 6.25000029685907e-05.\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 15.9710 - val_loss: 48.6231 - lr: 1.2500e-04\n",
      "Epoch 23/100\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 15.6164 - val_loss: 48.5801 - lr: 6.2500e-05\n",
      "Epoch 24/100\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 15.5634 - val_loss: 48.6586 - lr: 6.2500e-05\n",
      "Epoch 25/100\n",
      "46/46 [==============================] - ETA: 0s - loss: 15.5116\n",
      "Epoch 25: ReduceLROnPlateau reducing learning rate to 3.125000148429535e-05.\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 15.5116 - val_loss: 48.7174 - lr: 6.2500e-05\n",
      "Epoch 26/100\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 15.3788 - val_loss: 48.7140 - lr: 3.1250e-05\n",
      "Epoch 27/100\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 15.3166 - val_loss: 48.7229 - lr: 3.1250e-05\n",
      "Epoch 28/100\n",
      "46/46 [==============================] - ETA: 0s - loss: 15.3069\n",
      "Epoch 28: ReduceLROnPlateau reducing learning rate to 1.5625000742147677e-05.\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 15.3069 - val_loss: 48.6467 - lr: 3.1250e-05\n",
      "Epoch 29/100\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 15.3684 - val_loss: 48.6283 - lr: 1.5625e-05\n",
      "Epoch 30/100\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 15.2388 - val_loss: 48.6549 - lr: 1.5625e-05\n",
      "Epoch 31/100\n",
      "46/46 [==============================] - ETA: 0s - loss: 15.1638\n",
      "Epoch 31: ReduceLROnPlateau reducing learning rate to 7.812500371073838e-06.\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 15.1638 - val_loss: 48.6802 - lr: 1.5625e-05\n",
      "Epoch 32/100\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 15.1045 - val_loss: 48.6583 - lr: 7.8125e-06\n",
      "Epoch 33/100\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 15.3553 - val_loss: 48.6579 - lr: 7.8125e-06\n",
      "Epoch 34/100\n",
      "44/46 [===========================>..] - ETA: 0s - loss: 15.1602\n",
      "Epoch 34: ReduceLROnPlateau reducing learning rate to 3.906250185536919e-06.\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 15.1802 - val_loss: 48.6568 - lr: 7.8125e-06\n",
      "Epoch 35/100\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 15.1651 - val_loss: 48.6601 - lr: 3.9063e-06\n",
      "Epoch 36/100\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 15.1892 - val_loss: 48.6743 - lr: 3.9063e-06\n",
      "Epoch 37/100\n",
      "46/46 [==============================] - ETA: 0s - loss: 15.1325\n",
      "Epoch 37: ReduceLROnPlateau reducing learning rate to 1.9531250927684596e-06.\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 15.1325 - val_loss: 48.6699 - lr: 3.9063e-06\n",
      "Epoch 38/100\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 15.1871 - val_loss: 48.6665 - lr: 1.9531e-06\n",
      "Epoch 39/100\n",
      "46/46 [==============================] - 1s 12ms/step - loss: 15.1239 - val_loss: 48.6656 - lr: 1.9531e-06\n",
      "Epoch 40/100\n",
      "46/46 [==============================] - ETA: 0s - loss: 15.2372\n",
      "Epoch 40: ReduceLROnPlateau reducing learning rate to 9.765625463842298e-07.\n",
      "46/46 [==============================] - 1s 13ms/step - loss: 15.2372 - val_loss: 48.6558 - lr: 1.9531e-06\n",
      "Epoch 40: early stopping\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f6c0c43b150>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vae.add_loss(vae_loss)\n",
    "\n",
    "vae.compile(optimizer=Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False))\n",
    "\n",
    "# vae.compile(optimizer=SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True))\n",
    "\n",
    "# vae.summary()\n",
    "\n",
    "history = LossHistory()\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=30, verbose=1)\n",
    "\n",
    "learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', \n",
    "                                            patience=3, \n",
    "                                            verbose=1, \n",
    "                                            factor=0.5)\n",
    "\n",
    "\n",
    "vae.fit(\n",
    "    [data_train, attr_train],\n",
    "    shuffle=True,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    validation_data=([data_val, attr_val], None),\n",
    "    callbacks=[history, early_stopping, learning_rate_reduction]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    }
   ],
   "source": [
    "encoder.save(f'./model/{dataset}/encoder_{attr_type}.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate the averge attr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:No training configuration found in the save file, so the model was *not* compiled. Compile it manually.\n"
     ]
    }
   ],
   "source": [
    "# load encoder\n",
    "encoder = load_model(\n",
    "    f'./model/{dataset}/encoder_{attr_type}.h5', \n",
    "    custom_objects={'Scaler': Scaler, 'Sampling': Sampling}\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "seen(150): {0: 0, 1: 1, 8: 2, 9: 3, 13: 4, 15: 5, 30: 6, 31: 7, 46: 8, 47: 9, 48: 10, 51: 11, 52: 12, 53: 13, 54: 14, 63: 15, 64: 16, 70: 17, 108: 18, 109: 19, 150: 20, 155: 21, 160: 22, 161: 23, 179: 24, 84: 25, 77: 26, 60: 27, 153: 28, 34: 29, 126: 30, 25: 31, 36: 32, 76: 33, 198: 34, 130: 35, 21: 36, 145: 37, 167: 38, 74: 39, 82: 40, 187: 41, 96: 42, 29: 43, 189: 44, 163: 45, 118: 46, 89: 47, 98: 48, 88: 49, 19: 50, 139: 51, 199: 52, 197: 53, 116: 54, 44: 55, 27: 56, 3: 57, 195: 58, 102: 59, 174: 60, 117: 61, 58: 62, 91: 63, 5: 64, 105: 65, 157: 66, 193: 67, 39: 68, 176: 69, 114: 70, 4: 71, 101: 72, 180: 73, 171: 74, 73: 75, 142: 76, 2: 77, 172: 78, 133: 79, 120: 80, 144: 81, 146: 82, 32: 83, 23: 84, 72: 85, 162: 86, 22: 87, 106: 88, 59: 89, 40: 90, 104: 91, 148: 92, 66: 93, 147: 94, 169: 95, 69: 96, 168: 97, 110: 98, 11: 99, 111: 100, 81: 101, 41: 102, 17: 103, 16: 104, 56: 105, 10: 106, 131: 107, 38: 108, 14: 109, 143: 110, 37: 111, 24: 112, 112: 113, 62: 114, 7: 115, 83: 116, 185: 117, 134: 118, 42: 119, 113: 120, 65: 121, 164: 122, 100: 123, 136: 124, 177: 125, 127: 126, 50: 127, 26: 128, 196: 129, 132: 130, 12: 131, 45: 132, 154: 133, 182: 134, 43: 135, 152: 136, 57: 137, 135: 138, 75: 139, 85: 140, 80: 141, 137: 142, 122: 143, 92: 144, 95: 145, 183: 146, 125: 147, 93: 148, 129: 149}\n",
      "unseen(50): {165: 0, 78: 1, 156: 2, 178: 3, 35: 4, 186: 5, 99: 6, 86: 7, 190: 8, 87: 9, 149: 10, 170: 11, 128: 12, 20: 13, 184: 14, 192: 15, 181: 16, 194: 17, 55: 18, 6: 19, 28: 20, 140: 21, 141: 22, 188: 23, 115: 24, 33: 25, 159: 26, 121: 27, 158: 28, 103: 29, 67: 30, 49: 31, 68: 32, 175: 33, 71: 34, 90: 35, 119: 36, 166: 37, 124: 38, 107: 39, 79: 40, 61: 41, 191: 42, 97: 43, 138: 44, 173: 45, 18: 46, 151: 47, 94: 48, 123: 49}\n"
     ]
    }
   ],
   "source": [
    "train_set = set(label_train)\n",
    "val_set = set(label_val)\n",
    "trainval = list(train_set.union(val_set))\n",
    "\n",
    "all_classes = [classname[0] for classname in attr['allclasses_names'].flatten()]\n",
    "seen_classes = [line.rstrip() for line in open(f'./{file_path}/trainvalclasses.txt')]\n",
    "unseen_classes = [line.rstrip() for line in open(f'./{file_path}/testclasses.txt')]\n",
    "\n",
    "seen_label_map = {}\n",
    "unseen_label_map = {}\n",
    "\n",
    "for i in range(seen_class_num):\n",
    "    seen_label_map[all_classes.index(seen_classes[i])] = i\n",
    "\n",
    "for i in range(unseen_class_num):\n",
    "    unseen_label_map[all_classes.index(unseen_classes[i])] = i\n",
    "    \n",
    "print(f'seen({len(seen_label_map)}):', seen_label_map)\n",
    "print(f'unseen({len(unseen_label_map)}):', unseen_label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "276/276 [==============================] - 0s 988us/step\n",
      "(150, 312)\n"
     ]
    }
   ],
   "source": [
    "data_seen = np.vstack([data_train, data_val])\n",
    "label_seen = np.hstack([label_train, label_val])\n",
    "attr_seen = np.vstack([attr_train, attr_val])\n",
    "\n",
    "predict_attr = encoder.predict(data_seen)\n",
    "\n",
    "# sum 40 class attributes\n",
    "sum_attr = [[] for i in range(seen_class_num)]\n",
    "real_attr = [[] for i in range(seen_class_num)]\n",
    "count_class = [0 for i in range(seen_class_num)]\n",
    "\n",
    "for idx in range(len(predict_attr)):\n",
    "    original_label = label_seen[idx] - 1\n",
    "    map_label = seen_label_map[original_label]\n",
    "    if sum_attr[map_label] == []:\n",
    "        sum_attr[map_label] = predict_attr[idx].copy()\n",
    "        real_attr[map_label] = attr_seen[idx].copy()\n",
    "    else:    \n",
    "        sum_attr[map_label] += predict_attr[idx]\n",
    "    count_class[map_label] += 1\n",
    "\n",
    "# averge\n",
    "for i in range(seen_class_num):\n",
    "    sum_attr[i] = sum_attr[i] / count_class[i]\n",
    "sum_attr = np.array(sum_attr)\n",
    "print(sum_attr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot seen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class : 002.Laysan_Albatross, diff : 0.155502\n",
      "Class : 003.Sooty_Albatross, diff : 0.170338\n",
      "Class : 015.Lazuli_Bunting, diff : 0.15451\n",
      "Class : 016.Painted_Bunting, diff : 0.130965\n",
      "Class : 020.Yellow_breasted_Chat, diff : 0.202655\n",
      "Class : 022.Chuck_will_Widow, diff : 0.171639\n",
      "Class : 047.American_Goldfinch, diff : 0.136273\n",
      "Class : 048.European_Goldfinch, diff : 0.458967\n",
      "Class : 067.Anna_Hummingbird, diff : 0.194516\n",
      "Class : 068.Ruby_throated_Hummingbird, diff : 0.188331\n",
      "Class : 069.Rufous_Hummingbird, diff : 0.424068\n",
      "Class : 073.Blue_Jay, diff : 0.33249\n",
      "Class : 074.Florida_Jay, diff : 0.327598\n",
      "Class : 075.Green_Jay, diff : 0.135007\n",
      "Class : 076.Dark_eyed_Junco, diff : 0.321294\n",
      "Class : 089.Hooded_Merganser, diff : 0.324411\n",
      "Class : 090.Red_breasted_Merganser, diff : 0.13592\n",
      "Class : 100.Brown_Pelican, diff : 0.512492\n",
      "Class : 149.Brown_Thrasher, diff : 0.186056\n",
      "Class : 150.Sage_Thrasher, diff : 0.316805\n",
      "Class : 001.Black_footed_Albatross, diff : 0.149936\n",
      "Class : 014.Indigo_Bunting, diff : 0.148345\n",
      "Class : 034.Gray_crowned_Rosy_Finch, diff : 0.366755\n",
      "Class : 035.Purple_Finch, diff : 0.119176\n",
      "Class : 101.White_Pelican, diff : 0.138106\n",
      "Class : 120.Fox_Sparrow, diff : 0.160215\n",
      "Class : 110.Geococcyx, diff : 0.133721\n",
      "Class : 085.Horned_Lark, diff : 0.155771\n",
      "Class : 008.Rhinoceros_Auklet, diff : 0.176038\n",
      "Class : 054.Blue_Grosbeak, diff : 0.150236\n",
      "Class : 171.Myrtle_Warbler, diff : 0.172635\n",
      "Class : 041.Scissor_tailed_Flycatcher, diff : 0.156568\n",
      "Class : 056.Pine_Grosbeak, diff : 0.354254\n",
      "Class : 109.American_Redstart, diff : 0.280685\n",
      "Class : 187.American_Three_toed_Woodpecker, diff : 0.152044\n",
      "Class : 175.Pine_Warbler, diff : 0.186256\n",
      "Class : 030.Fish_Crow, diff : 0.132949\n",
      "Class : 195.Carolina_Wren, diff : 0.277348\n",
      "Class : 051.Horned_Grebe, diff : 0.299139\n",
      "Class : 107.Common_Raven, diff : 0.146521\n",
      "Class : 117.Clay_colored_Sparrow, diff : 0.237081\n",
      "Class : 135.Bank_Swallow, diff : 0.173435\n",
      "Class : 134.Cape_Glossy_Starling, diff : 0.147488\n",
      "Class : 046.Gadwall, diff : 0.13007\n",
      "Class : 147.Least_Tern, diff : 0.158369\n",
      "Class : 037.Acadian_Flycatcher, diff : 0.20878\n",
      "Class : 160.Black_throated_Blue_Warbler, diff : 0.16348\n",
      "Class : 126.Nelson_Sharp_tailed_Sparrow, diff : 0.192542\n",
      "Class : 137.Cliff_Swallow, diff : 0.190963\n",
      "Class : 125.Lincoln_Sparrow, diff : 0.198807\n",
      "Class : 027.Shiny_Cowbird, diff : 0.138199\n",
      "Class : 189.Red_bellied_Woodpecker, diff : 0.421319\n",
      "Class : 197.Marsh_Wren, diff : 0.156955\n",
      "Class : 186.Cedar_Waxwing, diff : 0.159324\n",
      "Class : 158.Bay_breasted_Warbler, diff : 0.335995\n",
      "Class : 064.Ring_billed_Gull, diff : 0.192341\n",
      "Class : 044.Frigatebird, diff : 0.140017\n",
      "Class : 007.Parakeet_Auklet, diff : 0.306705\n",
      "Class : 183.Northern_Waterthrush, diff : 0.170505\n",
      "Class : 142.Black_Tern, diff : 0.175782\n",
      "Class : 086.Pacific_Loon, diff : 0.321666\n",
      "Class : 159.Black_and_white_Warbler, diff : 0.146179\n",
      "Class : 081.Pied_Kingfisher, diff : 0.147043\n",
      "Class : 128.Seaside_Sparrow, diff : 0.322395\n",
      "Class : 011.Rusty_Blackbird, diff : 0.178014\n",
      "Class : 145.Elegant_Tern, diff : 0.156981\n",
      "Class : 029.American_Crow, diff : 0.147549\n",
      "Class : 166.Golden_winged_Warbler, diff : 0.162423\n",
      "Class : 059.California_Gull, diff : 0.208171\n",
      "Class : 095.Baltimore_Oriole, diff : 0.181373\n",
      "Class : 155.Warbling_Vireo, diff : 0.248848\n",
      "Class : 010.Red_winged_Blackbird, diff : 0.157589\n",
      "Class : 141.Artic_Tern, diff : 0.259588\n",
      "Class : 102.Western_Wood_Pewee, diff : 0.152126\n",
      "Class : 079.Belted_Kingfisher, diff : 0.177437\n",
      "Class : 106.Horned_Puffin, diff : 0.126187\n",
      "Class : 192.Downy_Woodpecker, diff : 0.283497\n",
      "Class : 005.Crested_Auklet, diff : 0.310543\n",
      "Class : 083.White_breasted_Kingfisher, diff : 0.548313\n",
      "Class : 178.Swainson_Warbler, diff : 0.180575\n",
      "Class : 162.Canada_Warbler, diff : 0.330865\n",
      "Class : 194.Cactus_Wren, diff : 0.329212\n",
      "Class : 196.House_Wren, diff : 0.162668\n",
      "Class : 050.Eared_Grebe, diff : 0.145065\n",
      "Class : 039.Least_Flycatcher, diff : 0.162655\n",
      "Class : 105.Whip_poor_Will, diff : 0.193996\n",
      "Class : 036.Northern_Flicker, diff : 0.431117\n",
      "Class : 032.Mangrove_Cuckoo, diff : 0.17433\n",
      "Class : 146.Forsters_Tern, diff : 0.147698\n",
      "Class : 082.Ringed_Kingfisher, diff : 0.1531\n",
      "Class : 060.Glaucous_winged_Gull, diff : 0.155721\n",
      "Class : 144.Common_Tern, diff : 0.190253\n",
      "Class : 199.Winter_Wren, diff : 0.156384\n",
      "Class : 093.Clark_Nutcracker, diff : 0.128444\n",
      "Class : 198.Rock_Wren, diff : 0.298518\n",
      "Class : 066.Western_Gull, diff : 0.234132\n",
      "Class : 099.Ovenbird, diff : 0.173867\n",
      "Class : 053.Western_Grebe, diff : 0.122067\n",
      "Class : 151.Black_capped_Vireo, diff : 0.311781\n",
      "Class : 018.Spotted_Catbird, diff : 0.538866\n",
      "Class : 152.Blue_headed_Vireo, diff : 0.17163\n",
      "Class : 116.Chipping_Sparrow, diff : 0.278806\n",
      "Class : 061.Heermann_Gull, diff : 0.296337\n",
      "Class : 025.Pelagic_Cormorant, diff : 0.158319\n",
      "Class : 024.Red_faced_Cormorant, diff : 0.163586\n",
      "Class : 078.Gray_Kingbird, diff : 0.181846\n",
      "Class : 017.Cardinal, diff : 0.146505\n",
      "Class : 176.Prairie_Warbler, diff : 0.174434\n",
      "Class : 058.Pigeon_Guillemot, diff : 0.152473\n",
      "Class : 021.Eastern_Towhee, diff : 0.153\n",
      "Class : 193.Bewick_Wren, diff : 0.195828\n",
      "Class : 057.Rose_breasted_Grosbeak, diff : 0.139043\n",
      "Class : 040.Olive_sided_Flycatcher, diff : 0.189095\n",
      "Class : 153.Philadelphia_Vireo, diff : 0.27271\n",
      "Class : 088.Western_Meadowlark, diff : 0.129136\n",
      "Class : 013.Bobolink, diff : 0.147304\n",
      "Class : 118.House_Sparrow, diff : 0.273278\n",
      "Class : 121.Grasshopper_Sparrow, diff : 0.238133\n",
      "Class : 179.Tennessee_Warbler, diff : 0.318511\n",
      "Class : 062.Herring_Gull, diff : 0.151199\n",
      "Class : 154.Red_eyed_Vireo, diff : 0.200047\n",
      "Class : 092.Nighthawk, diff : 0.161158\n",
      "Class : 038.Great_Crested_Flycatcher, diff : 0.184483\n",
      "Class : 140.Summer_Tanager, diff : 0.307704\n",
      "Class : 182.Yellow_Warbler, diff : 0.298075\n",
      "Class : 096.Hooded_Oriole, diff : 0.173234\n",
      "Class : 172.Nashville_Warbler, diff : 0.190617\n",
      "Class : 071.Long_tailed_Jaeger, diff : 0.177995\n",
      "Class : 042.Vermilion_Flycatcher, diff : 0.128249\n",
      "Class : 185.Bohemian_Waxwing, diff : 0.161944\n",
      "Class : 177.Prothonotary_Warbler, diff : 0.288331\n",
      "Class : 019.Gray_Catbird, diff : 0.155706\n",
      "Class : 065.Slaty_backed_Gull, diff : 0.189498\n",
      "Class : 009.Brewer_Blackbird, diff : 0.192063\n",
      "Class : 112.Great_Grey_Shrike, diff : 0.173133\n",
      "Class : 063.Ivory_Gull, diff : 0.303615\n",
      "Class : 006.Least_Auklet, diff : 0.164367\n",
      "Class : 080.Green_Kingfisher, diff : 0.344721\n",
      "Class : 181.Worm_eating_Warbler, diff : 0.154497\n",
      "Class : 108.White_necked_Raven, diff : 0.1691\n",
      "Class : 122.Harris_Sparrow, diff : 0.176453\n",
      "Class : 115.Brewer_Sparrow, diff : 0.185766\n",
      "Class : 184.Louisiana_Waterthrush, diff : 0.218929\n",
      "Class : 167.Hooded_Warbler, diff : 0.177673\n",
      "Class : 129.Song_Sparrow, diff : 0.164201\n",
      "Class : 133.White_throated_Sparrow, diff : 0.217869\n",
      "Class : 114.Black_throated_Sparrow, diff : 0.279949\n",
      "Class : 170.Mourning_Warbler, diff : 0.173694\n",
      "Class : 131.Vesper_Sparrow, diff : 0.168945\n",
      "Class : 174.Palm_Warbler, diff : 0.301671\n"
     ]
    }
   ],
   "source": [
    "for i in range(seen_class_num):\n",
    "    diff = round(np.sum(np.abs(sum_attr[i] - real_attr[i])) / len(real_attr[i]), 6)\n",
    "\n",
    "    if dataset == 'AWA2':\n",
    "        attributes_name = pd.read_csv(f'./{file_path}/predicates.txt', header=None, sep='\\t')\n",
    "        plt.figure(figsize=(40, 10))\n",
    "        plt.bar(attributes_name[1], height=sum_attr[i], align='edge', label='Learned CE', width=0.25)\n",
    "        plt.bar(attributes_name[1], height=real_attr[i], align='edge', label='Real CE', width=-0.25) # uncomment if you want show realCE\n",
    "        plt.legend(fontsize=15) # show label\n",
    "        plt.xlabel('Attributes', fontsize=30)\n",
    "        plt.xticks(fontsize=20, rotation='vertical')\n",
    "        plt.yticks(fontsize=20)\n",
    "        plt.title(f'Class : {seen_classes[i]}, diff : {diff}', fontsize=40)\n",
    "        plt.savefig(f'./data/{dataset}/mat/{attr_type}/seen/{seen_classes[i]}.jpg')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f'Class : {seen_classes[i]}, diff : {diff}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save the seen attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "seen_attr = sum_attr.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[239]\n",
      "(2967, 2048)\n",
      "(2967,)\n",
      "(2967, 312)\n",
      "195\n"
     ]
    }
   ],
   "source": [
    "test_loc = attr['test_unseen_loc']\n",
    "\n",
    "data_unseen = np.array([transpose_ft[i-1].flatten() for i in test_loc])\n",
    "label_unseen = np.array([int(features['labels'][i-1].flatten()) for i in test_loc])\n",
    "attr_unseen = np.array([process_attr[i-1] for i in label_unseen])\n",
    "\n",
    "print(data_unseen.shape)\n",
    "print(label_unseen.shape)\n",
    "print(attr_unseen.shape)\n",
    "print(max(label_unseen))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test unseen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "93/93 [==============================] - 0s 1ms/step\n",
      "(50, 312)\n"
     ]
    }
   ],
   "source": [
    "# calculate attr avg \n",
    "predict_attr = encoder.predict(data_unseen)\n",
    "\n",
    "# sum 10 class attributes\n",
    "sum_attr = [[] for i in range(unseen_class_num)]\n",
    "real_attr = [[] for i in range(unseen_class_num)]\n",
    "count_class = [0 for i in range(unseen_class_num)]\n",
    "\n",
    "for idx in range(len(predict_attr)):\n",
    "    original_label = label_unseen[idx] - 1\n",
    "    map_label = unseen_label_map[original_label]\n",
    "    if sum_attr[map_label] == []:\n",
    "        sum_attr[map_label] = predict_attr[idx].copy()\n",
    "        real_attr[map_label] = attr_unseen[idx].copy()\n",
    "    else:    \n",
    "        sum_attr[map_label] += predict_attr[idx]\n",
    "    count_class[map_label] += 1\n",
    "\n",
    "# averge\n",
    "for i in range(unseen_class_num):\n",
    "    sum_attr[i] = sum_attr[i] / count_class[i]\n",
    "sum_attr = np.array(sum_attr)\n",
    "print(sum_attr.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plot unseen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class : 043.Yellow_bellied_Flycatcher, diff : 0.255684\n",
      "Class : 111.Loggerhead_Shrike, diff : 0.206962\n",
      "Class : 023.Brandt_Cormorant, diff : 0.228082\n",
      "Class : 098.Scott_Oriole, diff : 0.282456\n",
      "Class : 055.Evening_Grosbeak, diff : 0.401304\n",
      "Class : 130.Tree_Sparrow, diff : 0.287108\n",
      "Class : 139.Scarlet_Tanager, diff : 0.283686\n",
      "Class : 123.Henslow_Sparrow, diff : 0.257288\n",
      "Class : 156.White_eyed_Vireo, diff : 0.258643\n",
      "Class : 124.Le_Conte_Sparrow, diff : 0.276093\n",
      "Class : 200.Common_Yellowthroat, diff : 0.366267\n",
      "Class : 072.Pomarine_Jaeger, diff : 0.256098\n",
      "Class : 173.Orange_crowned_Warbler, diff : 0.324896\n",
      "Class : 028.Brown_Creeper, diff : 0.417175\n",
      "Class : 119.Field_Sparrow, diff : 0.270681\n",
      "Class : 165.Chestnut_sided_Warbler, diff : 0.297096\n",
      "Class : 103.Sayornis, diff : 0.2857\n",
      "Class : 180.Wilson_Warbler, diff : 0.233478\n",
      "Class : 077.Tropical_Kingbird, diff : 0.248146\n",
      "Class : 012.Yellow_headed_Blackbird, diff : 0.347648\n",
      "Class : 045.Northern_Fulmar, diff : 0.269432\n",
      "Class : 190.Red_cockaded_Woodpecker, diff : 0.261675\n",
      "Class : 191.Red_headed_Woodpecker, diff : 0.399694\n",
      "Class : 138.Tree_Swallow, diff : 0.358256\n",
      "Class : 157.Yellow_throated_Vireo, diff : 0.284651\n",
      "Class : 052.Pied_billed_Grebe, diff : 0.412219\n",
      "Class : 033.Yellow_billed_Cuckoo, diff : 0.298059\n",
      "Class : 164.Cerulean_Warbler, diff : 0.302475\n",
      "Class : 031.Black_billed_Cuckoo, diff : 0.281301\n",
      "Class : 143.Caspian_Tern, diff : 0.214089\n",
      "Class : 094.White_breasted_Nuthatch, diff : 0.251719\n",
      "Class : 070.Green_Violetear, diff : 0.565413\n",
      "Class : 097.Orchard_Oriole, diff : 0.313493\n",
      "Class : 091.Mockingbird, diff : 0.251052\n",
      "Class : 104.American_Pipit, diff : 0.276163\n",
      "Class : 127.Savannah_Sparrow, diff : 0.235461\n",
      "Class : 161.Blue_winged_Warbler, diff : 0.247976\n",
      "Class : 049.Boat_tailed_Grackle, diff : 0.240802\n",
      "Class : 169.Magnolia_Warbler, diff : 0.317212\n",
      "Class : 148.Green_tailed_Towhee, diff : 0.441082\n",
      "Class : 113.Baird_Sparrow, diff : 0.242451\n",
      "Class : 087.Mallard, diff : 0.460654\n",
      "Class : 163.Cape_May_Warbler, diff : 0.305729\n",
      "Class : 136.Barn_Swallow, diff : 0.292532\n",
      "Class : 188.Pileated_Woodpecker, diff : 0.343789\n",
      "Class : 084.Red_legged_Kittiwake, diff : 0.288245\n",
      "Class : 026.Bronzed_Cowbird, diff : 0.192778\n",
      "Class : 004.Groove_billed_Ani, diff : 0.208371\n",
      "Class : 132.White_crowned_Sparrow, diff : 0.333184\n",
      "Class : 168.Kentucky_Warbler, diff : 0.25431\n"
     ]
    }
   ],
   "source": [
    "for i in range(unseen_class_num):\n",
    "    diff = round(np.sum(np.abs(sum_attr[i] - real_attr[i])) / len(real_attr[i]), 6)\n",
    "\n",
    "    if dataset == 'AWA2':\n",
    "        attributes_name = pd.read_csv(f'./{file_path}/predicates.txt', header=None, sep='\\t')\n",
    "        plt.figure(figsize=(40, 10))\n",
    "        plt.bar(attributes_name[1], height=sum_attr[i], align='edge', label='Learned CE', width=0.25)\n",
    "        plt.bar(attributes_name[1], height=real_attr[i], align='edge', label='Real CE', width=-0.25)\n",
    "        plt.legend(fontsize=15) # show label\n",
    "        plt.xlabel('Attributes', fontsize=30)\n",
    "        plt.xticks(fontsize=20, rotation='vertical')\n",
    "        plt.yticks(fontsize=20)\n",
    "        plt.title(f'Class : {unseen_classes[i]}, diff : {diff}', fontsize=40)\n",
    "        plt.savefig(f'./data/{dataset}/mat/{attr_type}/unseen/{unseen_classes[i]}.jpg')\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f'Class : {unseen_classes[i]}, diff : {diff}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## save the unseen attr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "unseen_attr = sum_attr.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_calculated_ce = np.vstack([seen_attr, unseen_attr])\n",
    "new_order_attr = [np.array([]) for i in range(class_num)]\n",
    "\n",
    "for k,v in unseen_label_map.items():\n",
    "    unseen_label_map[k] += seen_class_num\n",
    "\n",
    "all_label_map = {**seen_label_map, **unseen_label_map} # merge dict\n",
    "# print(all_label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k,v in all_label_map.items():\n",
    "    new_order_attr[k] = all_calculated_ce[v]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "attr['att'] = np.array(new_order_attr).transpose()\n",
    "sio.savemat(f'{file_path}/{mat_file}', attr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
